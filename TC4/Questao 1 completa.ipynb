{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3c3fcd-05a2-47d9-a20d-2b23d125c620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Leitura do arquivo e remoção da última coluna\n",
    "file = 'datasetTC4.dat'\n",
    "data = pd.read_csv(file, header=None, sep=' ')\n",
    "data = data.iloc[:, :-1]\n",
    "\n",
    "# Normalização dos dados\n",
    "scaler = StandardScaler()\n",
    "data_normalized = scaler.fit_transform(data)\n",
    "\n",
    "# Função para calcular o índice de Dunn\n",
    "def dunn_index(X, labels, centroids):\n",
    "    X_array = X.values if isinstance(X, pd.DataFrame) else X\n",
    "    dist_matrix = np.linalg.norm(X_array - centroids[labels], axis=-1)\n",
    "\n",
    "    min_diameter = np.inf\n",
    "    for k in range(len(centroids)):\n",
    "        cluster_points = X_array[labels == k]\n",
    "        cluster_diameter = np.max(np.linalg.norm(cluster_points - centroids[k], axis=-1))\n",
    "        min_diameter = min(min_diameter, cluster_diameter)\n",
    "\n",
    "    mask = labels != labels[:, np.newaxis]\n",
    "    min_intercluster_distance = np.min(dist_matrix[mask.any(axis=1)])\n",
    "\n",
    "    if min_diameter == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return min_intercluster_distance / min_diameter\n",
    "\n",
    "# Função para calcular o índice de Davies-Bouldin\n",
    "def davies_bouldin_index(X, labels, centroids):\n",
    "    X_array = X.values if isinstance(X, pd.DataFrame) else X\n",
    "    num_clusters = len(centroids)\n",
    "    distances_matrix = np.zeros((num_clusters, num_clusters))\n",
    "\n",
    "    for i in range(num_clusters):\n",
    "        for j in range(i + 1, num_clusters):\n",
    "            distances_matrix[i, j] = np.linalg.norm(centroids[i] - centroids[j])\n",
    "            distances_matrix[j, i] = distances_matrix[i, j]\n",
    "\n",
    "    max_cluster_distances = np.zeros(num_clusters)\n",
    "\n",
    "    for k in range(num_clusters):\n",
    "        cluster_points = X_array[labels == k]\n",
    "        mean_distance = np.mean(np.linalg.norm(cluster_points - centroids[k], axis=-1))\n",
    "        max_cluster_distances[k] = max([distances_matrix[k, j] for j in range(num_clusters) if j != k])\n",
    "\n",
    "    return np.sum(max_cluster_distances) / num_clusters\n",
    "\n",
    "# Função para calcular a métrica de Calinski-Harabasz\n",
    "def calinski_harabasz_index(X, labels, centroids):\n",
    "    X_array = X.values if isinstance(X, pd.DataFrame) else X\n",
    "    num_samples, num_features = X_array.shape\n",
    "    num_clusters = len(centroids)\n",
    "\n",
    "    overall_mean = np.mean(X_array, axis=0)\n",
    "    overall_ss = np.sum(np.sum((X_array - overall_mean) ** 2, axis=1))\n",
    "\n",
    "    between_cluster_ss = 0\n",
    "    within_cluster_ss = 0\n",
    "\n",
    "    for k in range(num_clusters):\n",
    "        cluster_points = X_array[labels == k]\n",
    "        cluster_size = len(cluster_points)\n",
    "\n",
    "        cluster_mean = np.mean(cluster_points, axis=0)\n",
    "        between_cluster_ss += cluster_size * np.sum((cluster_mean - overall_mean) ** 2)\n",
    "\n",
    "        within_cluster_ss += np.sum(np.sum((cluster_points - cluster_mean) ** 2, axis=1))\n",
    "\n",
    "    return (between_cluster_ss / (num_clusters - 1)) / (within_cluster_ss / (num_samples - num_clusters))\n",
    "\n",
    "# Realizar a análise para diferentes valores de K\n",
    "Kmax = 10\n",
    "\n",
    "# Inicializar listas para armazenar os resultados dos índices\n",
    "dunn_scores = []\n",
    "davies_bouldin_scores = []\n",
    "calinski_harabasz_scores = []\n",
    "\n",
    "for k in range(2, Kmax + 1):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=np.random.randint(1000))\n",
    "    labels = kmeans.fit_predict(data_normalized)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "\n",
    "    # Calcular os índices para cada valor de K\n",
    "    dunn_scores.append(dunn_index(data_normalized, labels, centroids))\n",
    "    davies_bouldin_scores.append(davies_bouldin_index(data_normalized, labels, centroids))\n",
    "    calinski_harabasz_scores.append(calinski_harabasz_index(data_normalized, labels, centroids))\n",
    "\n",
    "# Encontrar o valor ótimo de K para cada índice\n",
    "optimal_k_dunn = np.argmax(dunn_scores) + 2  # Adiciona 2 porque começamos com K=2\n",
    "optimal_k_davies_bouldin = np.argmin(davies_bouldin_scores) + 2\n",
    "optimal_k_calinski_harabasz = np.argmax(calinski_harabasz_scores) + 2\n",
    "\n",
    "# Imprimir os resultados\n",
    "print(\"Resultado do índice de Dunn:\")\n",
    "print(\" - Valor ótimo de K:\", optimal_k_dunn)\n",
    "print(\" - Índices de Dunn para diferentes valores de K:\", dunn_scores)\n",
    "\n",
    "print(\"\\nResultado do índice de Davies-Bouldin:\")\n",
    "print(\" - Valor ótimo de K:\", optimal_k_davies_bouldin)\n",
    "print(\" - Índices de Davies-Bouldin para diferentes valores de K:\", davies_bouldin_scores)\n",
    "\n",
    "print(\"\\nResultado do índice de Calinski-Harabasz:\")\n",
    "print(\" - Valor ótimo de K:\", optimal_k_calinski_harabasz)\n",
    "print(\" - Índices de Calinski-Harabasz para diferentes valores de K:\", calinski_harabasz_scores)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
